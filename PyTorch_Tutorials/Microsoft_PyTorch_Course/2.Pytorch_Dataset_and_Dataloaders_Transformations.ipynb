{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fae4a27",
   "metadata": {},
   "source": [
    "Source of this specific notebook: [Link](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/3-data?ns-enrollment-type=LearningPath&ns-enrollment-id=learn.pytorch.pytorch-fundamentals) and [Link2](https://docs.microsoft.com/en-us/learn/modules/intro-machine-learning-pytorch/4-transforms?ns-enrollment-type=LearningPath&ns-enrollment-id=learn.pytorch.pytorch-fundamentals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a750e6",
   "metadata": {},
   "source": [
    "# Datasets and Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90150043",
   "metadata": {},
   "source": [
    "Primary Purpose: <br>\n",
    "    - Make processing of data more maintainable/efficient <br>\n",
    "    - Decouple it well from model development codes <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6a7882",
   "metadata": {},
   "source": [
    "`Dataset`: (`torch.utils.data.Dataset`) <br>\n",
    "- stores samples and their corresponding labels <br>\n",
    "- Makes access to public datasets and your own datasets easier for model to access"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f408bd96",
   "metadata": {},
   "source": [
    "`Dataloader`: (`torch.utils.data.DataLoader`) <br>\n",
    "    - Wraps an `iterable` around the `Dataset` so that model can access well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa39e5",
   "metadata": {},
   "source": [
    "`Dataset` is the primary class where as `DataLoader` is a wrapper around `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5983952",
   "metadata": {},
   "source": [
    "### What are the list of torch classes/sub-packages and salient points covered in this notebook?\n",
    " - `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`\n",
    "     - Helps in processing of data easier and efficient for models\n",
    "     - `Dataset` retrieves features and labels one sample at a time <br>\n",
    "     - What does `DataLoaders` do? <br>\n",
    "         - For model training, <br>\n",
    "         - we need data fed in `minibatches` <br>\n",
    "         - reshuffle the data (to reduce model overfitting)\n",
    "         - `multiprocessing` to speed up data retrieval \n",
    " <br>\n",
    " <br>\n",
    " - `torchvision.datasets` --> to use pre-existing datasets like FashionMNIST, coco, cifar, etc.,\n",
    "     - torchvision.datasets have arguments/parameters to `transform` featuers (aka inputs) and `target_transform` to transform labels (like one hot encoding of labels\n",
    " - CustomDatasetClass must overwrite the `magic methods` of python such as \n",
    "     - `__init__`, `__getitem__` and `__len__` methods inherited from `Dataset`\n",
    " - `torchvision.transforms.ToTensor` (to transform/modify the features) and `torchvision.transforms.Lambda` (to transform the target/labels)\n",
    "     - `torchvision.transforms.ToTensor()` converts features to normalized tensors\n",
    "     - `torchvision.transforms.Lambda` could be used to transform labels\n",
    "         - ```python\n",
    "         \n",
    "         Lambda(lambda y: torch.zeros(number_of_classes,dtype=torch.float).scatter_(dim=0, index=torch.tensor(y), value=1)\n",
    "                  )`\n",
    "           ```\n",
    "          - `Tensor.scatter_` is used to change values of a tensor variable at specified indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96578ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2500ac7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset # why is this needed here? `Dataset` class is inherited inside a `CustomDataset` calss\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bbb8980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dataset in module torch.utils.data.dataset:\n",
      "\n",
      "class Dataset(typing.Generic)\n",
      " |  Dataset(*args, **kwds)\n",
      " |  \n",
      " |  An abstract class representing a :class:`Dataset`.\n",
      " |  \n",
      " |  All datasets that represent a map from keys to data samples should subclass\n",
      " |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      " |  data sample for a given key. Subclasses could also optionally overwrite\n",
      " |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      " |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      " |  of :class:`~torch.utils.data.DataLoader`.\n",
      " |  \n",
      " |  .. note::\n",
      " |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      " |    sampler that yields integral indices.  To make it work with a map-style\n",
      " |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  __getitem__(self, index) -> +T_co\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  __parameters__ = (+T_co,)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bc8ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torchvision.datasets in torchvision:\n",
      "\n",
      "NAME\n",
      "    torchvision.datasets\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _utils\n",
      "    caltech\n",
      "    celeba\n",
      "    cifar\n",
      "    cityscapes\n",
      "    coco\n",
      "    fakedata\n",
      "    flickr\n",
      "    folder\n",
      "    hmdb51\n",
      "    imagenet\n",
      "    kinetics\n",
      "    kitti\n",
      "    lsun\n",
      "    mnist\n",
      "    omniglot\n",
      "    phototour\n",
      "    places365\n",
      "    samplers (package)\n",
      "    sbd\n",
      "    sbu\n",
      "    semeion\n",
      "    stl10\n",
      "    svhn\n",
      "    ucf101\n",
      "    usps\n",
      "    utils\n",
      "    video_utils\n",
      "    vision\n",
      "    voc\n",
      "    widerface\n",
      "\n",
      "CLASSES\n",
      "    torch.utils.data.dataset.Dataset(typing.Generic)\n",
      "        torchvision.datasets.vision.VisionDataset\n",
      "            torchvision.datasets.caltech.Caltech101\n",
      "            torchvision.datasets.caltech.Caltech256\n",
      "            torchvision.datasets.celeba.CelebA\n",
      "            torchvision.datasets.cifar.CIFAR10\n",
      "                torchvision.datasets.cifar.CIFAR100\n",
      "            torchvision.datasets.cityscapes.Cityscapes\n",
      "            torchvision.datasets.coco.CocoDetection\n",
      "                torchvision.datasets.coco.CocoCaptions\n",
      "            torchvision.datasets.fakedata.FakeData\n",
      "            torchvision.datasets.flickr.Flickr30k\n",
      "            torchvision.datasets.flickr.Flickr8k\n",
      "            torchvision.datasets.folder.DatasetFolder\n",
      "                torchvision.datasets.folder.ImageFolder\n",
      "                    torchvision.datasets.imagenet.ImageNet\n",
      "            torchvision.datasets.hmdb51.HMDB51\n",
      "            torchvision.datasets.kinetics.Kinetics400\n",
      "            torchvision.datasets.kitti.Kitti\n",
      "            torchvision.datasets.lsun.LSUN\n",
      "            torchvision.datasets.lsun.LSUNClass\n",
      "            torchvision.datasets.mnist.MNIST\n",
      "                torchvision.datasets.mnist.EMNIST\n",
      "                torchvision.datasets.mnist.FashionMNIST\n",
      "                torchvision.datasets.mnist.KMNIST\n",
      "                torchvision.datasets.mnist.QMNIST\n",
      "            torchvision.datasets.omniglot.Omniglot\n",
      "            torchvision.datasets.phototour.PhotoTour\n",
      "            torchvision.datasets.places365.Places365\n",
      "            torchvision.datasets.sbd.SBDataset\n",
      "            torchvision.datasets.sbu.SBU\n",
      "            torchvision.datasets.semeion.SEMEION\n",
      "            torchvision.datasets.stl10.STL10\n",
      "            torchvision.datasets.svhn.SVHN\n",
      "            torchvision.datasets.ucf101.UCF101\n",
      "            torchvision.datasets.usps.USPS\n",
      "            torchvision.datasets.widerface.WIDERFace\n",
      "    torchvision.datasets.voc._VOCBase(torchvision.datasets.vision.VisionDataset)\n",
      "        torchvision.datasets.voc.VOCDetection\n",
      "        torchvision.datasets.voc.VOCSegmentation\n",
      "    \n",
      "    class CIFAR10(torchvision.datasets.vision.VisionDataset)\n",
      "     |  CIFAR10(*args, **kwds)\n",
      "     |  \n",
      "     |  `CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory\n",
      "     |          ``cifar-10-batches-py`` exists or will be saved to if download is set to True.\n",
      "     |      train (bool, optional): If True, creates dataset from training set, otherwise\n",
      "     |          creates from test set.\n",
      "     |      transform (callable, optional): A function/transform that takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CIFAR10\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  base_folder = 'cifar-10-batches-py'\n",
      "     |  \n",
      "     |  filename = 'cifar-10-python.tar.gz'\n",
      "     |  \n",
      "     |  meta = {'filename': 'batches.meta', 'key': 'label_names', 'md5': '5ff9...\n",
      "     |  \n",
      "     |  test_list = [['test_batch', '40351d587109b95175f43aff81a1287e']]\n",
      "     |  \n",
      "     |  tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
      "     |  \n",
      "     |  train_list = [['data_batch_1', 'c99cafc152244af753f735de768cd75f'], ['...\n",
      "     |  \n",
      "     |  url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class CIFAR100(CIFAR10)\n",
      "     |  CIFAR100(*args, **kwds)\n",
      "     |  \n",
      "     |  `CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
      "     |  \n",
      "     |  This is a subclass of the `CIFAR10` Dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CIFAR100\n",
      "     |      CIFAR10\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  base_folder = 'cifar-100-python'\n",
      "     |  \n",
      "     |  filename = 'cifar-100-python.tar.gz'\n",
      "     |  \n",
      "     |  meta = {'filename': 'meta', 'key': 'fine_label_names', 'md5': '7973b15...\n",
      "     |  \n",
      "     |  test_list = [['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc']]\n",
      "     |  \n",
      "     |  tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
      "     |  \n",
      "     |  train_list = [['train', '16019d7e3df5f24257cddd939b257f8d']]\n",
      "     |  \n",
      "     |  url = 'https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CIFAR10:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Caltech101(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Caltech101(*args, **kwds)\n",
      "     |  \n",
      "     |  `Caltech 101 <http://www.vision.caltech.edu/Image_Datasets/Caltech101/>`_ Dataset.\n",
      "     |  \n",
      "     |  .. warning::\n",
      "     |  \n",
      "     |      This class needs `scipy <https://docs.scipy.org/doc/>`_ to load target files from `.mat` format.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory\n",
      "     |          ``caltech101`` exists or will be saved to if download is set to True.\n",
      "     |      target_type (string or list, optional): Type of target to use, ``category`` or\n",
      "     |      ``annotation``. Can also be a list to output a tuple with all specified target types.\n",
      "     |      ``category`` represents the target class, and ``annotation`` is a list of points\n",
      "     |      from a hand-generated outline. Defaults to ``category``.\n",
      "     |      transform (callable, optional): A function/transform that takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Caltech101\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where the type of target specified by target_type.\n",
      "     |  \n",
      "     |  __init__(self, root: str, target_type: Union[List[str], str] = 'category', transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Caltech256(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Caltech256(*args, **kwds)\n",
      "     |  \n",
      "     |  `Caltech 256 <http://www.vision.caltech.edu/Image_Datasets/Caltech256/>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory\n",
      "     |          ``caltech256`` exists or will be saved to if download is set to True.\n",
      "     |      transform (callable, optional): A function/transform that takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Caltech256\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class CelebA(torchvision.datasets.vision.VisionDataset)\n",
      "     |  CelebA(*args, **kwds)\n",
      "     |  \n",
      "     |  `Large-scale CelebFaces Attributes (CelebA) Dataset <http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images are downloaded to.\n",
      "     |      split (string): One of {'train', 'valid', 'test', 'all'}.\n",
      "     |          Accordingly dataset is selected.\n",
      "     |      target_type (string or list, optional): Type of target to use, ``attr``, ``identity``, ``bbox``,\n",
      "     |          or ``landmarks``. Can also be a list to output a tuple with all specified target types.\n",
      "     |          The targets represent:\n",
      "     |  \n",
      "     |              - ``attr`` (np.array shape=(40,) dtype=int): binary (0, 1) labels for attributes\n",
      "     |              - ``identity`` (int): label for each person (data points with the same identity are the same person)\n",
      "     |              - ``bbox`` (np.array shape=(4,) dtype=int): bounding box (x, y, width, height)\n",
      "     |              - ``landmarks`` (np.array shape=(10,) dtype=int): landmark points (lefteye_x, lefteye_y, righteye_x,\n",
      "     |                righteye_y, nose_x, nose_y, leftmouth_x, leftmouth_y, rightmouth_x, rightmouth_y)\n",
      "     |  \n",
      "     |          Defaults to ``attr``. If empty, ``None`` will be returned as target.\n",
      "     |  \n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.ToTensor``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CelebA\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str = 'train', target_type: Union[List[str], str] = 'attr', transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  base_folder = 'celeba'\n",
      "     |  \n",
      "     |  file_list = [('0B7EVK8r0v71pZjFTYXZWM3FlRnM', '00d2c5bc6d35e252742224a...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Cityscapes(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Cityscapes(*args, **kwds)\n",
      "     |  \n",
      "     |  `Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory ``leftImg8bit``\n",
      "     |          and ``gtFine`` or ``gtCoarse`` are located.\n",
      "     |      split (string, optional): The image split to use, ``train``, ``test`` or ``val`` if mode=\"fine\"\n",
      "     |          otherwise ``train``, ``train_extra`` or ``val``\n",
      "     |      mode (string, optional): The quality mode to use, ``fine`` or ``coarse``\n",
      "     |      target_type (string or list, optional): Type of target to use, ``instance``, ``semantic``, ``polygon``\n",
      "     |          or ``color``. Can also be a list to output a tuple with all specified target types.\n",
      "     |      transform (callable, optional): A function/transform that takes in a PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |      Get semantic segmentation target\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n",
      "     |                               target_type='semantic')\n",
      "     |  \n",
      "     |          img, smnt = dataset[0]\n",
      "     |  \n",
      "     |      Get multiple targets\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n",
      "     |                               target_type=['instance', 'color', 'polygon'])\n",
      "     |  \n",
      "     |          img, (inst, col, poly) = dataset[0]\n",
      "     |  \n",
      "     |      Validate on the \"coarse\" set\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          dataset = Cityscapes('./data/cityscapes', split='val', mode='coarse',\n",
      "     |                               target_type='semantic')\n",
      "     |  \n",
      "     |          img, smnt = dataset[0]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Cityscapes\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
      "     |          than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str = 'train', mode: str = 'fine', target_type: Union[List[str], str] = 'instance', transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, transforms: Union[Callable, NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  CityscapesClass = <class 'torchvision.datasets.cityscapes.CityscapesCl...\n",
      "     |      CityscapesClass(name, id, train_id, category, category_id, has_instances, ignore_in_eval, color)\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  classes = [CityscapesClass(name='unlabeled', id=0, train_id...nces=Fal...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class CocoCaptions(CocoDetection)\n",
      "     |  CocoCaptions(*args, **kwds)\n",
      "     |  \n",
      "     |  `MS Coco Captions <https://cocodataset.org/#captions-2015>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images are downloaded to.\n",
      "     |      annFile (string): Path to json annotation file.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.ToTensor``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      .. code:: python\n",
      "     |  \n",
      "     |          import torchvision.datasets as dset\n",
      "     |          import torchvision.transforms as transforms\n",
      "     |          cap = dset.CocoCaptions(root = 'dir where images are',\n",
      "     |                                  annFile = 'json annotation file',\n",
      "     |                                  transform=transforms.ToTensor())\n",
      "     |  \n",
      "     |          print('Number of samples: ', len(cap))\n",
      "     |          img, target = cap[3] # load 4th sample\n",
      "     |  \n",
      "     |          print(\"Image Size: \", img.size())\n",
      "     |          print(target)\n",
      "     |  \n",
      "     |      Output: ::\n",
      "     |  \n",
      "     |          Number of samples: 82783\n",
      "     |          Image Size: (3L, 427L, 640L)\n",
      "     |          [u'A plane emitting smoke stream flying over a mountain.',\n",
      "     |          u'A plane darts across a bright blue sky behind a mountain covered in snow',\n",
      "     |          u'A plane leaves a contrail above the snowy mountain top.',\n",
      "     |          u'A mountain that has a plane flying overheard in the distance.',\n",
      "     |          u'A mountain view with a plume of smoke in the background']\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CocoCaptions\n",
      "     |      CocoDetection\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from CocoDetection:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, root: str, annFile: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, transforms: Union[Callable, NoneType] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class CocoDetection(torchvision.datasets.vision.VisionDataset)\n",
      "     |  CocoDetection(*args, **kwds)\n",
      "     |  \n",
      "     |  `MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images are downloaded to.\n",
      "     |      annFile (string): Path to json annotation file.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.ToTensor``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CocoDetection\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, root: str, annFile: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, transforms: Union[Callable, NoneType] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class DatasetFolder(torchvision.datasets.vision.VisionDataset)\n",
      "     |  DatasetFolder(*args, **kwds)\n",
      "     |  \n",
      "     |  A generic data loader.\n",
      "     |  \n",
      "     |  This default directory structure can be customized by overriding the\n",
      "     |  :meth:`find_classes` method.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory path.\n",
      "     |      loader (callable): A function to load a sample given its path.\n",
      "     |      extensions (tuple[string]): A list of allowed extensions.\n",
      "     |          both extensions and is_valid_file should not be passed.\n",
      "     |      transform (callable, optional): A function/transform that takes in\n",
      "     |          a sample and returns a transformed version.\n",
      "     |          E.g, ``transforms.RandomCrop`` for images.\n",
      "     |      target_transform (callable, optional): A function/transform that takes\n",
      "     |          in the target and transforms it.\n",
      "     |      is_valid_file (callable, optional): A function that takes path of a file\n",
      "     |          and check if the file is a valid file (used to check of corrupt files)\n",
      "     |          both extensions and is_valid_file should not be passed.\n",
      "     |  \n",
      "     |   Attributes:\n",
      "     |      classes (list): List of the class names sorted alphabetically.\n",
      "     |      class_to_idx (dict): Dict with items (class_name, class_index).\n",
      "     |      samples (list): List of (sample path, class_index) tuples\n",
      "     |      targets (list): The class_index value for each image in the dataset\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DatasetFolder\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (sample, target) where target is class_index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, loader: Callable[[str], Any], extensions: Union[Tuple[str, ...], NoneType] = None, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, is_valid_file: Union[Callable[[str], bool], NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]\n",
      "     |      Find the class folders in a dataset structured as follows::\n",
      "     |      \n",
      "     |          directory/\n",
      "     |           class_x\n",
      "     |              xxx.ext\n",
      "     |              xxy.ext\n",
      "     |              ...\n",
      "     |                  xxz.ext\n",
      "     |           class_y\n",
      "     |               123.ext\n",
      "     |               nsdf3.ext\n",
      "     |               ...\n",
      "     |               asd932_.ext\n",
      "     |      \n",
      "     |      This method can be overridden to only consider\n",
      "     |      a subset of classes, or to adapt to a different dataset directory structure.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          directory(str): Root directory path, corresponding to ``self.root``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          FileNotFoundError: If ``dir`` has no class folders.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  make_dataset(directory: str, class_to_idx: Dict[str, int], extensions: Union[Tuple[str, ...], NoneType] = None, is_valid_file: Union[Callable[[str], bool], NoneType] = None) -> List[Tuple[str, int]]\n",
      "     |      Generates a list of samples of a form (path_to_sample, class).\n",
      "     |      \n",
      "     |      This can be overridden to e.g. read files from a compressed zip file instead of from the disk.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          directory (str): root dataset directory, corresponding to ``self.root``.\n",
      "     |          class_to_idx (Dict[str, int]): Dictionary mapping class name to class index.\n",
      "     |          extensions (optional): A list of allowed extensions.\n",
      "     |              Either extensions or is_valid_file should be passed. Defaults to None.\n",
      "     |          is_valid_file (optional): A function that takes path of a file\n",
      "     |              and checks if the file is a valid file\n",
      "     |              (used to check of corrupt files) both extensions and\n",
      "     |              is_valid_file should not be passed. Defaults to None.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: In case ``class_to_idx`` is empty.\n",
      "     |          ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.\n",
      "     |          FileNotFoundError: In case no valid file was found for any class.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          List[Tuple[str, int]]: samples of a form (path_to_sample, class)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class EMNIST(MNIST)\n",
      "     |  EMNIST(*args, **kwds)\n",
      "     |  \n",
      "     |  `EMNIST <https://www.westernsydney.edu.au/bens/home/reproducible_research/emnist>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where ``EMNIST/processed/training.pt``\n",
      "     |          and  ``EMNIST/processed/test.pt`` exist.\n",
      "     |      split (string): The dataset has 6 different splits: ``byclass``, ``bymerge``,\n",
      "     |          ``balanced``, ``letters``, ``digits`` and ``mnist``. This argument specifies\n",
      "     |          which one to use.\n",
      "     |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "     |          otherwise from ``test.pt``.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EMNIST\n",
      "     |      MNIST\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str, **kwargs: Any) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |      Download the EMNIST data if it doesn't exist already.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  images_file\n",
      "     |  \n",
      "     |  labels_file\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  classes_split_dict = {'balanced': ['0', '1', '2', '3', '4', '5', '6', ...\n",
      "     |  \n",
      "     |  md5 = '58c8d27c78d21e728a6bc7b3cc06412e'\n",
      "     |  \n",
      "     |  splits = ('byclass', 'bymerge', 'balanced', 'letters', 'digits', 'mnis...\n",
      "     |  \n",
      "     |  url = 'https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MNIST:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MNIST:\n",
      "     |  \n",
      "     |  class_to_idx\n",
      "     |  \n",
      "     |  processed_folder\n",
      "     |  \n",
      "     |  raw_folder\n",
      "     |  \n",
      "     |  test_data\n",
      "     |  \n",
      "     |  test_labels\n",
      "     |  \n",
      "     |  train_data\n",
      "     |  \n",
      "     |  train_labels\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from MNIST:\n",
      "     |  \n",
      "     |  classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', ...\n",
      "     |  \n",
      "     |  mirrors = ['http://yann.lecun.com/exdb/mnist/', 'https://ossci-dataset...\n",
      "     |  \n",
      "     |  resources = [('train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbd...\n",
      "     |  \n",
      "     |  test_file = 'test.pt'\n",
      "     |  \n",
      "     |  training_file = 'training.pt'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class FakeData(torchvision.datasets.vision.VisionDataset)\n",
      "     |  FakeData(*args, **kwds)\n",
      "     |  \n",
      "     |  A fake dataset that returns randomly generated images and returns them as PIL images\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      size (int, optional): Size of the dataset. Default: 1000 images\n",
      "     |      image_size(tuple, optional): Size if the returned images. Default: (3, 224, 224)\n",
      "     |      num_classes(int, optional): Number of classes in the dataset. Default: 10\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      random_offset (int): Offsets the index-based random seed used to\n",
      "     |          generate each image. Default: 0\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FakeData\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is class_index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, size: int = 1000, image_size: Tuple[int, int, int] = (3, 224, 224), num_classes: int = 10, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, random_offset: int = 0) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class FashionMNIST(MNIST)\n",
      "     |  FashionMNIST(*args, **kwds)\n",
      "     |  \n",
      "     |  `Fashion-MNIST <https://github.com/zalandoresearch/fashion-mnist>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where ``FashionMNIST/processed/training.pt``\n",
      "     |          and  ``FashionMNIST/processed/test.pt`` exist.\n",
      "     |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "     |          otherwise from ``test.pt``.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FashionMNIST\n",
      "     |      MNIST\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'San...\n",
      "     |  \n",
      "     |  mirrors = ['http://fashion-mnist.s3-website.eu-central-1.amazonaws.com...\n",
      "     |  \n",
      "     |  resources = [('train-images-idx3-ubyte.gz', '8d4fb7e6c68d591d4c3dfef9e...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MNIST:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |      Download the MNIST data if it doesn't exist already.\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MNIST:\n",
      "     |  \n",
      "     |  class_to_idx\n",
      "     |  \n",
      "     |  processed_folder\n",
      "     |  \n",
      "     |  raw_folder\n",
      "     |  \n",
      "     |  test_data\n",
      "     |  \n",
      "     |  test_labels\n",
      "     |  \n",
      "     |  train_data\n",
      "     |  \n",
      "     |  train_labels\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from MNIST:\n",
      "     |  \n",
      "     |  test_file = 'test.pt'\n",
      "     |  \n",
      "     |  training_file = 'training.pt'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Flickr30k(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Flickr30k(*args, **kwds)\n",
      "     |  \n",
      "     |  `Flickr30k Entities <http://web.engr.illinois.edu/~bplumme2/Flickr30kEntities/>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images are downloaded to.\n",
      "     |      ann_file (string): Path to annotation file.\n",
      "     |      transform (callable, optional): A function/transform that takes in a PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.ToTensor``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Flickr30k\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: Tuple (image, target). target is a list of captions for the image.\n",
      "     |  \n",
      "     |  __init__(self, root: str, ann_file: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Flickr8k(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Flickr8k(*args, **kwds)\n",
      "     |  \n",
      "     |  `Flickr8k Entities <http://hockenmaier.cs.illinois.edu/8k-pictures.html>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images are downloaded to.\n",
      "     |      ann_file (string): Path to annotation file.\n",
      "     |      transform (callable, optional): A function/transform that takes in a PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.ToTensor``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Flickr8k\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: Tuple (image, target). target is a list of captions for the image.\n",
      "     |  \n",
      "     |  __init__(self, root: str, ann_file: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class HMDB51(torchvision.datasets.vision.VisionDataset)\n",
      "     |  HMDB51(*args, **kwds)\n",
      "     |  \n",
      "     |  `HMDB51 <http://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/>`_\n",
      "     |  dataset.\n",
      "     |  \n",
      "     |  HMDB51 is an action recognition video dataset.\n",
      "     |  This dataset consider every video as a collection of video clips of fixed size, specified\n",
      "     |  by ``frames_per_clip``, where the step in frames between each clip is given by\n",
      "     |  ``step_between_clips``.\n",
      "     |  \n",
      "     |  To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5``\n",
      "     |  and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two\n",
      "     |  elements will come from video 1, and the next three elements from video 2.\n",
      "     |  Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all\n",
      "     |  frames in a video might be present.\n",
      "     |  \n",
      "     |  Internally, it uses a VideoClips object to handle clip creation.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the HMDB51 Dataset.\n",
      "     |      annotation_path (str): Path to the folder containing the split files.\n",
      "     |      frames_per_clip (int): Number of frames in a clip.\n",
      "     |      step_between_clips (int): Number of frames between each clip.\n",
      "     |      fold (int, optional): Which fold to use. Should be between 1 and 3.\n",
      "     |      train (bool, optional): If ``True``, creates a dataset from the train split,\n",
      "     |          otherwise from the ``test`` split.\n",
      "     |      transform (callable, optional): A function/transform that takes in a TxHxWxC video\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Returns:\n",
      "     |      tuple: A 3-tuple with the following entries:\n",
      "     |  \n",
      "     |          - video (Tensor[T, H, W, C]): The `T` video frames\n",
      "     |          - audio(Tensor[K, L]): the audio frames, where `K` is the number of channels\n",
      "     |            and `L` is the number of points\n",
      "     |          - label (int): class of the video clip\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HMDB51\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, root, annotation_path, frames_per_clip, step_between_clips=1, frame_rate=None, fold=1, train=True, transform=None, _precomputed_metadata=None, num_workers=1, _video_width=0, _video_height=0, _video_min_dimension=0, _audio_samples=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  metadata\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  TEST_TAG = 2\n",
      "     |  \n",
      "     |  TRAIN_TAG = 1\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  data_url = 'http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10...\n",
      "     |  \n",
      "     |  splits = {'md5': '15e67781e70dcfbdce2d7dbb9b3344b5', 'url': 'http://se...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class ImageFolder(DatasetFolder)\n",
      "     |  ImageFolder(*args, **kwds)\n",
      "     |  \n",
      "     |  A generic data loader where the images are arranged in this way by default: ::\n",
      "     |  \n",
      "     |      root/dog/xxx.png\n",
      "     |      root/dog/xxy.png\n",
      "     |      root/dog/[...]/xxz.png\n",
      "     |  \n",
      "     |      root/cat/123.png\n",
      "     |      root/cat/nsdf3.png\n",
      "     |      root/cat/[...]/asd932_.png\n",
      "     |  \n",
      "     |  This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n",
      "     |  the same methods can be overridden to customize the dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory path.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      loader (callable, optional): A function to load an image given its path.\n",
      "     |      is_valid_file (callable, optional): A function that takes path of an Image file\n",
      "     |          and check if the file is a valid file (used to check of corrupt files)\n",
      "     |  \n",
      "     |   Attributes:\n",
      "     |      classes (list): List of the class names sorted alphabetically.\n",
      "     |      class_to_idx (dict): Dict with items (class_name, class_index).\n",
      "     |      imgs (list): List of (image path, class_index) tuples\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ImageFolder\n",
      "     |      DatasetFolder\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, loader: Callable[[str], Any] = <function default_loader at 0x7fc4feba7560>, is_valid_file: Union[Callable[[str], bool], NoneType] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DatasetFolder:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (sample, target) where target is class_index of the target class.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]\n",
      "     |      Find the class folders in a dataset structured as follows::\n",
      "     |      \n",
      "     |          directory/\n",
      "     |           class_x\n",
      "     |              xxx.ext\n",
      "     |              xxy.ext\n",
      "     |              ...\n",
      "     |                  xxz.ext\n",
      "     |           class_y\n",
      "     |               123.ext\n",
      "     |               nsdf3.ext\n",
      "     |               ...\n",
      "     |               asd932_.ext\n",
      "     |      \n",
      "     |      This method can be overridden to only consider\n",
      "     |      a subset of classes, or to adapt to a different dataset directory structure.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          directory(str): Root directory path, corresponding to ``self.root``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          FileNotFoundError: If ``dir`` has no class folders.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from DatasetFolder:\n",
      "     |  \n",
      "     |  make_dataset(directory: str, class_to_idx: Dict[str, int], extensions: Union[Tuple[str, ...], NoneType] = None, is_valid_file: Union[Callable[[str], bool], NoneType] = None) -> List[Tuple[str, int]]\n",
      "     |      Generates a list of samples of a form (path_to_sample, class).\n",
      "     |      \n",
      "     |      This can be overridden to e.g. read files from a compressed zip file instead of from the disk.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          directory (str): root dataset directory, corresponding to ``self.root``.\n",
      "     |          class_to_idx (Dict[str, int]): Dictionary mapping class name to class index.\n",
      "     |          extensions (optional): A list of allowed extensions.\n",
      "     |              Either extensions or is_valid_file should be passed. Defaults to None.\n",
      "     |          is_valid_file (optional): A function that takes path of a file\n",
      "     |              and checks if the file is a valid file\n",
      "     |              (used to check of corrupt files) both extensions and\n",
      "     |              is_valid_file should not be passed. Defaults to None.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: In case ``class_to_idx`` is empty.\n",
      "     |          ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.\n",
      "     |          FileNotFoundError: In case no valid file was found for any class.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          List[Tuple[str, int]]: samples of a form (path_to_sample, class)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class ImageNet(torchvision.datasets.folder.ImageFolder)\n",
      "     |  ImageNet(*args, **kwds)\n",
      "     |  \n",
      "     |  `ImageNet <http://image-net.org/>`_ 2012 Classification Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the ImageNet Dataset.\n",
      "     |      split (string, optional): The dataset split, supports ``train``, or ``val``.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      loader (callable, optional): A function to load an image given its path.\n",
      "     |  \n",
      "     |   Attributes:\n",
      "     |      classes (list): List of the class name tuples.\n",
      "     |      class_to_idx (dict): Dict with items (class_name, class_index).\n",
      "     |      wnids (list): List of the WordNet IDs.\n",
      "     |      wnid_to_idx (dict): Dict with items (wordnet_id, class_index).\n",
      "     |      imgs (list): List of (image path, class_index) tuples\n",
      "     |      targets (list): The class_index value for each image in the dataset\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ImageNet\n",
      "     |      torchvision.datasets.folder.ImageFolder\n",
      "     |      torchvision.datasets.folder.DatasetFolder\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str = 'train', download: Union[str, NoneType] = None, **kwargs: Any) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  parse_archives(self) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  split_folder\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.folder.DatasetFolder:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (sample, target) where target is class_index of the target class.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  find_classes(self, directory: str) -> Tuple[List[str], Dict[str, int]]\n",
      "     |      Find the class folders in a dataset structured as follows::\n",
      "     |      \n",
      "     |          directory/\n",
      "     |           class_x\n",
      "     |              xxx.ext\n",
      "     |              xxy.ext\n",
      "     |              ...\n",
      "     |                  xxz.ext\n",
      "     |           class_y\n",
      "     |               123.ext\n",
      "     |               nsdf3.ext\n",
      "     |               ...\n",
      "     |               asd932_.ext\n",
      "     |      \n",
      "     |      This method can be overridden to only consider\n",
      "     |      a subset of classes, or to adapt to a different dataset directory structure.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          directory(str): Root directory path, corresponding to ``self.root``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          FileNotFoundError: If ``dir`` has no class folders.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from torchvision.datasets.folder.DatasetFolder:\n",
      "     |  \n",
      "     |  make_dataset(directory: str, class_to_idx: Dict[str, int], extensions: Union[Tuple[str, ...], NoneType] = None, is_valid_file: Union[Callable[[str], bool], NoneType] = None) -> List[Tuple[str, int]]\n",
      "     |      Generates a list of samples of a form (path_to_sample, class).\n",
      "     |      \n",
      "     |      This can be overridden to e.g. read files from a compressed zip file instead of from the disk.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          directory (str): root dataset directory, corresponding to ``self.root``.\n",
      "     |          class_to_idx (Dict[str, int]): Dictionary mapping class name to class index.\n",
      "     |          extensions (optional): A list of allowed extensions.\n",
      "     |              Either extensions or is_valid_file should be passed. Defaults to None.\n",
      "     |          is_valid_file (optional): A function that takes path of a file\n",
      "     |              and checks if the file is a valid file\n",
      "     |              (used to check of corrupt files) both extensions and\n",
      "     |              is_valid_file should not be passed. Defaults to None.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: In case ``class_to_idx`` is empty.\n",
      "     |          ValueError: In case ``extensions`` and ``is_valid_file`` are None or both are not None.\n",
      "     |          FileNotFoundError: In case no valid file was found for any class.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          List[Tuple[str, int]]: samples of a form (path_to_sample, class)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class KMNIST(MNIST)\n",
      "     |  KMNIST(*args, **kwds)\n",
      "     |  \n",
      "     |  `Kuzushiji-MNIST <https://github.com/rois-codh/kmnist>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where ``KMNIST/processed/training.pt``\n",
      "     |          and  ``KMNIST/processed/test.pt`` exist.\n",
      "     |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "     |          otherwise from ``test.pt``.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KMNIST\n",
      "     |      MNIST\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  classes = ['o', 'ki', 'su', 'tsu', 'na', 'ha', 'ma', 'ya', 're', 'wo']\n",
      "     |  \n",
      "     |  mirrors = ['http://codh.rois.ac.jp/kmnist/dataset/kmnist/']\n",
      "     |  \n",
      "     |  resources = [('train-images-idx3-ubyte.gz', 'bdb82020997e1d708af4cf47b...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MNIST:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |      Download the MNIST data if it doesn't exist already.\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MNIST:\n",
      "     |  \n",
      "     |  class_to_idx\n",
      "     |  \n",
      "     |  processed_folder\n",
      "     |  \n",
      "     |  raw_folder\n",
      "     |  \n",
      "     |  test_data\n",
      "     |  \n",
      "     |  test_labels\n",
      "     |  \n",
      "     |  train_data\n",
      "     |  \n",
      "     |  train_labels\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from MNIST:\n",
      "     |  \n",
      "     |  test_file = 'test.pt'\n",
      "     |  \n",
      "     |  training_file = 'training.pt'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Kinetics400(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Kinetics400(*args, **kwds)\n",
      "     |  \n",
      "     |  `Kinetics-400 <https://deepmind.com/research/open-source/open-source-datasets/kinetics/>`_\n",
      "     |  dataset.\n",
      "     |  \n",
      "     |  Kinetics-400 is an action recognition video dataset.\n",
      "     |  This dataset consider every video as a collection of video clips of fixed size, specified\n",
      "     |  by ``frames_per_clip``, where the step in frames between each clip is given by\n",
      "     |  ``step_between_clips``.\n",
      "     |  \n",
      "     |  To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5``\n",
      "     |  and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two\n",
      "     |  elements will come from video 1, and the next three elements from video 2.\n",
      "     |  Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all\n",
      "     |  frames in a video might be present.\n",
      "     |  \n",
      "     |  Internally, it uses a VideoClips object to handle clip creation.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the Kinetics-400 Dataset. Should be structured as follows:\n",
      "     |  \n",
      "     |          .. code::\n",
      "     |  \n",
      "     |              root/\n",
      "     |               class1\n",
      "     |                  clip1.avi\n",
      "     |                  clip2.avi\n",
      "     |                  ...\n",
      "     |               class2\n",
      "     |                   clipx.avi\n",
      "     |                   ...\n",
      "     |  \n",
      "     |      frames_per_clip (int): number of frames in a clip\n",
      "     |      step_between_clips (int): number of frames between each clip\n",
      "     |      transform (callable, optional): A function/transform that  takes in a TxHxWxC video\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Returns:\n",
      "     |      tuple: A 3-tuple with the following entries:\n",
      "     |  \n",
      "     |          - video (Tensor[T, H, W, C]): the `T` video frames\n",
      "     |          - audio(Tensor[K, L]): the audio frames, where `K` is the number of channels\n",
      "     |            and `L` is the number of points\n",
      "     |          - label (int): class of the video clip\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Kinetics400\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, root, frames_per_clip, step_between_clips=1, frame_rate=None, extensions=('avi',), transform=None, _precomputed_metadata=None, num_workers=1, _video_width=0, _video_height=0, _video_min_dimension=0, _audio_samples=0, _audio_channels=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  metadata\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Kitti(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Kitti(*args, **kwds)\n",
      "     |  \n",
      "     |  `KITTI <http://www.cvlibs.net/datasets/kitti>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images are downloaded to.\n",
      "     |          Expects the following folder structure if download=False:\n",
      "     |  \n",
      "     |          .. code::\n",
      "     |  \n",
      "     |              <root>\n",
      "     |                   Kitti\n",
      "     |                       raw\n",
      "     |                           training\n",
      "     |                          |    image_2\n",
      "     |                          |    label_2\n",
      "     |                           testing\n",
      "     |                               image_2\n",
      "     |      train (bool, optional): Use ``train`` split if true, else ``test`` split.\n",
      "     |          Defaults to ``train``.\n",
      "     |      transform (callable, optional): A function/transform that takes in a PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.ToTensor``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      transforms (callable, optional): A function/transform that takes input sample\n",
      "     |          and its target as entry and returns a transformed version.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Kitti\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Get item at a given index.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      Returns:\n",
      "     |          tuple: (image, target), where\n",
      "     |          target is a list of dictionaries with the following keys:\n",
      "     |      \n",
      "     |          - type: str\n",
      "     |          - truncated: float\n",
      "     |          - occluded: int\n",
      "     |          - alpha: float\n",
      "     |          - bbox: float[4]\n",
      "     |          - dimensions: float[3]\n",
      "     |          - locations: float[3]\n",
      "     |          - rotation_y: float\n",
      "     |  \n",
      "     |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, transforms: Union[Callable, NoneType] = None, download: bool = False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |      Download the KITTI data if it doesn't exist already.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  data_url = 'https://s3.eu-central-1.amazonaws.com/avg-kitti/'\n",
      "     |  \n",
      "     |  image_dir_name = 'image_2'\n",
      "     |  \n",
      "     |  labels_dir_name = 'label_2'\n",
      "     |  \n",
      "     |  resources = ['data_object_image_2.zip', 'data_object_label_2.zip']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class LSUN(torchvision.datasets.vision.VisionDataset)\n",
      "     |  LSUN(*args, **kwds)\n",
      "     |  \n",
      "     |  `LSUN <https://www.yf.io/p/lsun>`_ dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory for the database files.\n",
      "     |      classes (string or list): One of {'train', 'val', 'test'} or a list of\n",
      "     |          categories to load. e,g. ['bedroom_train', 'church_outdoor_train'].\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LSUN\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: Tuple (image, target) where target is the index of the target category.\n",
      "     |  \n",
      "     |  __init__(self, root: str, classes: Union[str, List[str]] = 'train', transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class LSUNClass(torchvision.datasets.vision.VisionDataset)\n",
      "     |  LSUNClass(*args, **kwds)\n",
      "     |  \n",
      "     |  An abstract class representing a :class:`Dataset`.\n",
      "     |  \n",
      "     |  All datasets that represent a map from keys to data samples should subclass\n",
      "     |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      "     |  data sample for a given key. Subclasses could also optionally overwrite\n",
      "     |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      "     |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      "     |  of :class:`~torch.utils.data.DataLoader`.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      "     |    sampler that yields integral indices.  To make it work with a map-style\n",
      "     |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LSUNClass\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, root: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class MNIST(torchvision.datasets.vision.VisionDataset)\n",
      "     |  MNIST(*args, **kwds)\n",
      "     |  \n",
      "     |  `MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where ``MNIST/processed/training.pt``\n",
      "     |          and  ``MNIST/processed/test.pt`` exist.\n",
      "     |      train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "     |          otherwise from ``test.pt``.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MNIST\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |      Download the MNIST data if it doesn't exist already.\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  class_to_idx\n",
      "     |  \n",
      "     |  processed_folder\n",
      "     |  \n",
      "     |  raw_folder\n",
      "     |  \n",
      "     |  test_data\n",
      "     |  \n",
      "     |  test_labels\n",
      "     |  \n",
      "     |  train_data\n",
      "     |  \n",
      "     |  train_labels\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', ...\n",
      "     |  \n",
      "     |  mirrors = ['http://yann.lecun.com/exdb/mnist/', 'https://ossci-dataset...\n",
      "     |  \n",
      "     |  resources = [('train-images-idx3-ubyte.gz', 'f68b3c2dcbeaaa9fbdd348bbd...\n",
      "     |  \n",
      "     |  test_file = 'test.pt'\n",
      "     |  \n",
      "     |  training_file = 'training.pt'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Omniglot(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Omniglot(*args, **kwds)\n",
      "     |  \n",
      "     |  `Omniglot <https://github.com/brendenlake/omniglot>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory\n",
      "     |          ``omniglot-py`` exists.\n",
      "     |      background (bool, optional): If True, creates dataset from the \"background\" set, otherwise\n",
      "     |          creates from the \"evaluation\" set. This terminology is defined by the authors.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset zip files from the internet and\n",
      "     |          puts it in root directory. If the zip files are already downloaded, they are not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Omniglot\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target character class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, background: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  download_url_prefix = 'https://raw.githubusercontent.com/brendenlake/o...\n",
      "     |  \n",
      "     |  folder = 'omniglot-py'\n",
      "     |  \n",
      "     |  zips_md5 = {'images_background': '68d2efa1b9178cc56df9314c21c6e718', '...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class PhotoTour(torchvision.datasets.vision.VisionDataset)\n",
      "     |  PhotoTour(*args, **kwds)\n",
      "     |  \n",
      "     |  `Multi-view Stereo Correspondence <http://matthewalunbrown.com/patchdata/patchdata.html>`_ Dataset.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      We only provide the newer version of the dataset, since the authors state that it\n",
      "     |  \n",
      "     |          is more suitable for training descriptors based on difference of Gaussian, or Harris corners, as the\n",
      "     |          patches are centred on real interest point detections, rather than being projections of 3D points as is the\n",
      "     |          case in the old dataset.\n",
      "     |  \n",
      "     |      The original dataset is available under http://phototour.cs.washington.edu/patches/default.htm.\n",
      "     |  \n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images are.\n",
      "     |      name (string): Name of the dataset to load.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PhotoTour\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Union[torch.Tensor, Tuple[Any, Any, torch.Tensor]]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (data1, data2, matches)\n",
      "     |  \n",
      "     |  __init__(self, root: str, name: str, train: bool = True, transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  cache(self) -> None\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  image_ext = 'bmp'\n",
      "     |  \n",
      "     |  info_file = 'info.txt'\n",
      "     |  \n",
      "     |  lens = {'liberty': 450092, 'liberty_harris': 379587, 'notredame': 4681...\n",
      "     |  \n",
      "     |  matches_files = 'm50_100000_100000_0.txt'\n",
      "     |  \n",
      "     |  means = {'liberty': 0.4437, 'liberty_harris': 0.4437, 'notredame': 0.4...\n",
      "     |  \n",
      "     |  stds = {'liberty': 0.2019, 'liberty_harris': 0.2019, 'notredame': 0.18...\n",
      "     |  \n",
      "     |  urls = {'liberty': ['http://icvl.ee.ic.ac.uk/vbalnt/liberty.zip', 'lib...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Places365(torchvision.datasets.vision.VisionDataset)\n",
      "     |  Places365(*args, **kwds)\n",
      "     |  \n",
      "     |  `Places365 <http://places2.csail.mit.edu/index.html>`_ classification dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the Places365 dataset.\n",
      "     |      split (string, optional): The dataset split. Can be one of ``train-standard`` (default), ``train-challenge``,\n",
      "     |          ``val``.\n",
      "     |      small (bool, optional): If ``True``, uses the small images, i. e. resized to 256 x 256 pixels, instead of the\n",
      "     |          high resolution ones.\n",
      "     |      download (bool, optional): If ``True``, downloads the dataset components and places them in ``root``. Already\n",
      "     |          downloaded archives are not downloaded again.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      loader (callable, optional): A function to load an image given its path.\n",
      "     |  \n",
      "     |   Attributes:\n",
      "     |      classes (list): List of the class names.\n",
      "     |      class_to_idx (dict): Dict with items (class_name, class_index).\n",
      "     |      imgs (list): List of (image path, class_index) tuples\n",
      "     |      targets (list): The class_index value for each image in the dataset\n",
      "     |  \n",
      "     |  Raises:\n",
      "     |      RuntimeError: If ``download is False`` and the meta files, i. e. the devkit, are not present or corrupted.\n",
      "     |      RuntimeError: If ``download is True`` and the image archive is already extracted.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Places365\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str = 'train-standard', small: bool = False, download: bool = False, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, loader: Callable[[str], Any] = <function default_loader at 0x7fc4feba7560>) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download_devkit(self) -> None\n",
      "     |  \n",
      "     |  download_images(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  load_categories(self, download: bool = True) -> Tuple[List[str], Dict[str, int]]\n",
      "     |  \n",
      "     |  load_file_list(self, download: bool = True) -> Tuple[List[Tuple[str, int]], List[int]]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  images_dir\n",
      "     |  \n",
      "     |  variant\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class QMNIST(MNIST)\n",
      "     |  QMNIST(*args, **kwds)\n",
      "     |  \n",
      "     |  `QMNIST <https://github.com/facebookresearch/qmnist>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset whose ``processed``\n",
      "     |          subdir contains torch binary files with the datasets.\n",
      "     |      what (string,optional): Can be 'train', 'test', 'test10k',\n",
      "     |          'test50k', or 'nist' for respectively the mnist compatible\n",
      "     |          training set, the 60k qmnist testing set, the 10k qmnist\n",
      "     |          examples that match the mnist testing set, the 50k\n",
      "     |          remaining qmnist testing examples, or all the nist\n",
      "     |          digits. The default is to select 'train' or 'test'\n",
      "     |          according to the compatibility argument 'train'.\n",
      "     |      compat (bool,optional): A boolean that says whether the target\n",
      "     |          for each example is class number (for compatibility with\n",
      "     |          the MNIST dataloader) or a torch vector containing the\n",
      "     |          full qmnist information. Default=True.\n",
      "     |      download (bool, optional): If true, downloads the dataset from\n",
      "     |          the internet and puts it in root directory. If dataset is\n",
      "     |          already downloaded, it is not downloaded again.\n",
      "     |      transform (callable, optional): A function/transform that\n",
      "     |          takes in an PIL image and returns a transformed\n",
      "     |          version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform\n",
      "     |          that takes in the target and transforms it.\n",
      "     |      train (bool,optional,compatibility): When argument 'what' is\n",
      "     |          not specified, this boolean decides whether to load the\n",
      "     |          training set ot the testing set.  Default: True.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      QMNIST\n",
      "     |      MNIST\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, what: Union[str, NoneType] = None, compat: bool = True, train: bool = True, **kwargs: Any) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |      Download the QMNIST data if it doesn't exist already.\n",
      "     |      Note that we only download what has been asked for (argument 'what').\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  images_file\n",
      "     |  \n",
      "     |  labels_file\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'resources': typing.Dict[str, typing.List[typing.Tu...\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  classes = ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', ...\n",
      "     |  \n",
      "     |  resources = {'nist': [('https://raw.githubusercontent.com/facebookrese...\n",
      "     |  \n",
      "     |  subsets = {'nist': 'nist', 'test': 'test', 'test10k': 'test', 'test50k...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MNIST:\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from MNIST:\n",
      "     |  \n",
      "     |  class_to_idx\n",
      "     |  \n",
      "     |  processed_folder\n",
      "     |  \n",
      "     |  raw_folder\n",
      "     |  \n",
      "     |  test_data\n",
      "     |  \n",
      "     |  test_labels\n",
      "     |  \n",
      "     |  train_data\n",
      "     |  \n",
      "     |  train_labels\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from MNIST:\n",
      "     |  \n",
      "     |  mirrors = ['http://yann.lecun.com/exdb/mnist/', 'https://ossci-dataset...\n",
      "     |  \n",
      "     |  test_file = 'test.pt'\n",
      "     |  \n",
      "     |  training_file = 'training.pt'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class SBDataset(torchvision.datasets.vision.VisionDataset)\n",
      "     |  SBDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  `Semantic Boundaries Dataset <http://home.bharathh.info/pubs/codes/SBD/download.html>`_\n",
      "     |  \n",
      "     |  The SBD currently contains annotations from 11355 images taken from the PASCAL VOC 2011 dataset.\n",
      "     |  \n",
      "     |  .. note ::\n",
      "     |  \n",
      "     |      Please note that the train and val splits included with this dataset are different from\n",
      "     |      the splits in the PASCAL VOC dataset. In particular some \"train\" images might be part of\n",
      "     |      VOC2012 val.\n",
      "     |      If you are interested in testing on VOC 2012 val, then use `image_set='train_noval'`,\n",
      "     |      which excludes all val images.\n",
      "     |  \n",
      "     |  .. warning::\n",
      "     |  \n",
      "     |      This class needs `scipy <https://docs.scipy.org/doc/>`_ to load target files from `.mat` format.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the Semantic Boundaries Dataset\n",
      "     |      image_set (string, optional): Select the image_set to use, ``train``, ``val`` or ``train_noval``.\n",
      "     |          Image set ``train_noval`` excludes VOC 2012 val images.\n",
      "     |      mode (string, optional): Select target type. Possible values 'boundaries' or 'segmentation'.\n",
      "     |          In case of 'boundaries', the target is an array of shape `[num_classes, H, W]`,\n",
      "     |          where `num_classes=20`.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |      transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
      "     |          and returns a transformed version. Input sample is PIL image and target is a numpy array\n",
      "     |          if `mode='boundaries'` or PIL image if `mode='segmentation'`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SBDataset\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |  \n",
      "     |  __init__(self, root: str, image_set: str = 'train', mode: str = 'boundaries', download: bool = False, transforms: Union[Callable, NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  filename = 'benchmark.tgz'\n",
      "     |  \n",
      "     |  md5 = '82b4d87ceb2ed10f6038a1cba92111cb'\n",
      "     |  \n",
      "     |  url = 'https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grou...\n",
      "     |  \n",
      "     |  voc_split_filename = 'train_noval.txt'\n",
      "     |  \n",
      "     |  voc_split_md5 = '79bff800c5f0b1ec6b21080a3c066722'\n",
      "     |  \n",
      "     |  voc_train_url = 'http://home.bharathh.info/pubs/codes/SBD/train_noval....\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class SBU(torchvision.datasets.vision.VisionDataset)\n",
      "     |  SBU(*args, **kwds)\n",
      "     |  \n",
      "     |  `SBU Captioned Photo <http://www.cs.virginia.edu/~vicente/sbucaptions/>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where tarball\n",
      "     |          ``SBUCaptionedPhotoDataset.tar.gz`` exists.\n",
      "     |      transform (callable, optional): A function/transform that takes in a PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If True, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SBU\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is a caption for the photo.\n",
      "     |  \n",
      "     |  __init__(self, root: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = True) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |      The number of photos in the dataset.\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |      Download and extract the tarball, and download each individual photo.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  filename = 'SBUCaptionedPhotoDataset.tar.gz'\n",
      "     |  \n",
      "     |  md5_checksum = '9aec147b3488753cf758b4d493422285'\n",
      "     |  \n",
      "     |  url = 'http://www.cs.virginia.edu/~vicente/sbucaptions/SBUCaptionedPho...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class SEMEION(torchvision.datasets.vision.VisionDataset)\n",
      "     |  SEMEION(*args, **kwds)\n",
      "     |  \n",
      "     |  `SEMEION <http://archive.ics.uci.edu/ml/datasets/semeion+handwritten+digit>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory\n",
      "     |          ``semeion.py`` exists.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SEMEION\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = True) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  filename = 'semeion.data'\n",
      "     |  \n",
      "     |  md5_checksum = 'cb545d371d2ce14ec121470795a77432'\n",
      "     |  \n",
      "     |  url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/semeio...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class STL10(torchvision.datasets.vision.VisionDataset)\n",
      "     |  STL10(*args, **kwds)\n",
      "     |  \n",
      "     |  `STL10 <https://cs.stanford.edu/~acoates/stl10/>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory\n",
      "     |          ``stl10_binary`` exists.\n",
      "     |      split (string): One of {'train', 'test', 'unlabeled', 'train+unlabeled'}.\n",
      "     |          Accordingly dataset is selected.\n",
      "     |      folds (int, optional): One of {0-9} or None.\n",
      "     |          For training, loads one of the 10 pre-defined folds of 1k samples for the\n",
      "     |          standard evaluation procedure. If no value is passed, loads the 5k samples.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      STL10\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str = 'train', folds: Union[int, NoneType] = None, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  base_folder = 'stl10_binary'\n",
      "     |  \n",
      "     |  class_names_file = 'class_names.txt'\n",
      "     |  \n",
      "     |  filename = 'stl10_binary.tar.gz'\n",
      "     |  \n",
      "     |  folds_list_file = 'fold_indices.txt'\n",
      "     |  \n",
      "     |  splits = ('train', 'train+unlabeled', 'unlabeled', 'test')\n",
      "     |  \n",
      "     |  test_list = [['test_X.bin', '7f263ba9f9e0b06b93213547f721ac82'], ['tes...\n",
      "     |  \n",
      "     |  tgz_md5 = '91f7769df0f17e558f3565bffb0c7dfb'\n",
      "     |  \n",
      "     |  train_list = [['train_X.bin', '918c2871b30a85fa023e0c44e0bee87f'], ['t...\n",
      "     |  \n",
      "     |  url = 'http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class SVHN(torchvision.datasets.vision.VisionDataset)\n",
      "     |  SVHN(*args, **kwds)\n",
      "     |  \n",
      "     |  `SVHN <http://ufldl.stanford.edu/housenumbers/>`_ Dataset.\n",
      "     |  Note: The SVHN dataset assigns the label `10` to the digit `0`. However, in this Dataset,\n",
      "     |  we assign the label `0` to the digit `0` to be compatible with PyTorch loss functions which\n",
      "     |  expect the class labels to be in the range `[0, C-1]`\n",
      "     |  \n",
      "     |  .. warning::\n",
      "     |  \n",
      "     |      This class needs `scipy <https://docs.scipy.org/doc/>`_ to load data from `.mat` format.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset where directory\n",
      "     |          ``SVHN`` exists.\n",
      "     |      split (string): One of {'train', 'test', 'extra'}.\n",
      "     |          Accordingly dataset is selected. 'extra' is Extra training set.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SVHN\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str = 'train', transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  split_list = {'extra': ['http://ufldl.stanford.edu/housenumbers/extra_...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class UCF101(torchvision.datasets.vision.VisionDataset)\n",
      "     |  UCF101(*args, **kwds)\n",
      "     |  \n",
      "     |  `UCF101 <https://www.crcv.ucf.edu/data/UCF101.php>`_ dataset.\n",
      "     |  \n",
      "     |  UCF101 is an action recognition video dataset.\n",
      "     |  This dataset consider every video as a collection of video clips of fixed size, specified\n",
      "     |  by ``frames_per_clip``, where the step in frames between each clip is given by\n",
      "     |  ``step_between_clips``.\n",
      "     |  \n",
      "     |  To give an example, for 2 videos with 10 and 15 frames respectively, if ``frames_per_clip=5``\n",
      "     |  and ``step_between_clips=5``, the dataset size will be (2 + 3) = 5, where the first two\n",
      "     |  elements will come from video 1, and the next three elements from video 2.\n",
      "     |  Note that we drop clips which do not have exactly ``frames_per_clip`` elements, so not all\n",
      "     |  frames in a video might be present.\n",
      "     |  \n",
      "     |  Internally, it uses a VideoClips object to handle clip creation.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the UCF101 Dataset.\n",
      "     |      annotation_path (str): path to the folder containing the split files\n",
      "     |      frames_per_clip (int): number of frames in a clip.\n",
      "     |      step_between_clips (int, optional): number of frames between each clip.\n",
      "     |      fold (int, optional): which fold to use. Should be between 1 and 3.\n",
      "     |      train (bool, optional): if ``True``, creates a dataset from the train split,\n",
      "     |          otherwise from the ``test`` split.\n",
      "     |      transform (callable, optional): A function/transform that  takes in a TxHxWxC video\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Returns:\n",
      "     |      tuple: A 3-tuple with the following entries:\n",
      "     |  \n",
      "     |          - video (Tensor[T, H, W, C]): the `T` video frames\n",
      "     |          -  audio(Tensor[K, L]): the audio frames, where `K` is the number of channels\n",
      "     |             and `L` is the number of points\n",
      "     |          - label (int): class of the video clip\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UCF101\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, root, annotation_path, frames_per_clip, step_between_clips=1, frame_rate=None, fold=1, train=True, transform=None, _precomputed_metadata=None, num_workers=1, _video_width=0, _video_height=0, _video_min_dimension=0, _audio_samples=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  metadata\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class USPS(torchvision.datasets.vision.VisionDataset)\n",
      "     |  USPS(*args, **kwds)\n",
      "     |  \n",
      "     |  `USPS <https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html#usps>`_ Dataset.\n",
      "     |  The data-format is : [label [index:value ]*256 \\n] * num_lines, where ``label`` lies in ``[1, 10]``.\n",
      "     |  The value for each pixel lies in ``[-1, 1]``. Here we transform the ``label`` into ``[0, 9]``\n",
      "     |  and make pixel values in ``[0, 255]``.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of dataset to store``USPS`` data files.\n",
      "     |      train (bool, optional): If True, creates dataset from ``usps.bz2``,\n",
      "     |          otherwise from ``usps.t.bz2``.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      USPS\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is index of the target class.\n",
      "     |  \n",
      "     |  __init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  split_list = {'test': ['https://www.csie.ntu.edu.tw/~cjlin/libsvmtools...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class VOCDetection(_VOCBase)\n",
      "     |  VOCDetection(*args, **kwds)\n",
      "     |  \n",
      "     |  `Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Detection Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the VOC Dataset.\n",
      "     |      year (string, optional): The dataset year, supports years ``\"2007\"`` to ``\"2012\"``.\n",
      "     |      image_set (string, optional): Select the image_set to use, ``\"train\"``, ``\"trainval\"`` or ``\"val\"``. If\n",
      "     |          ``year==\"2007\"``, can also be ``\"test\"``.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |          (default: alphabetic indexing of VOC's 20 classes).\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, required): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VOCDetection\n",
      "     |      _VOCBase\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is a dictionary of the XML tree.\n",
      "     |  \n",
      "     |  parse_voc_xml(self, node: xml.etree.ElementTree.Element) -> Dict[str, Any]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  annotations\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _VOCBase:\n",
      "     |  \n",
      "     |  __init__(self, root: str, year: str = '2012', image_set: str = 'train', download: bool = False, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, transforms: Union[Callable, NoneType] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from _VOCBase:\n",
      "     |  \n",
      "     |  __annotations__ = {'_SPLITS_DIR': <class 'str'>, '_TARGET_DIR': <class...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class VOCSegmentation(_VOCBase)\n",
      "     |  VOCSegmentation(*args, **kwds)\n",
      "     |  \n",
      "     |  `Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Segmentation Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory of the VOC Dataset.\n",
      "     |      year (string, optional): The dataset year, supports years ``\"2007\"`` to ``\"2012\"``.\n",
      "     |      image_set (string, optional): Select the image_set to use, ``\"train\"``, ``\"trainval\"`` or ``\"val\"``. If\n",
      "     |          ``year==\"2007\"``, can also be ``\"test\"``.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |      transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
      "     |          and returns a transformed version.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VOCSegmentation\n",
      "     |      _VOCBase\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is the image segmentation.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  masks\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _VOCBase:\n",
      "     |  \n",
      "     |  __init__(self, root: str, year: str = '2012', image_set: str = 'train', download: bool = False, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, transforms: Union[Callable, NoneType] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from _VOCBase:\n",
      "     |  \n",
      "     |  __annotations__ = {'_SPLITS_DIR': <class 'str'>, '_TARGET_DIR': <class...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class VisionDataset(torch.utils.data.dataset.Dataset)\n",
      "     |  VisionDataset(*args, **kwds)\n",
      "     |  \n",
      "     |  An abstract class representing a :class:`Dataset`.\n",
      "     |  \n",
      "     |  All datasets that represent a map from keys to data samples should subclass\n",
      "     |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      "     |  data sample for a given key. Subclasses could also optionally overwrite\n",
      "     |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      "     |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      "     |  of :class:`~torch.utils.data.DataLoader`.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      "     |    sampler that yields integral indices.  To make it work with a map-style\n",
      "     |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Any\n",
      "     |  \n",
      "     |  __init__(self, root: str, transforms: Union[Callable, NoneType] = None, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class WIDERFace(torchvision.datasets.vision.VisionDataset)\n",
      "     |  WIDERFace(*args, **kwds)\n",
      "     |  \n",
      "     |  `WIDERFace <http://shuoyang1213.me/WIDERFACE/>`_ Dataset.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      root (string): Root directory where images and annotations are downloaded to.\n",
      "     |          Expects the following folder structure if download=False:\n",
      "     |  \n",
      "     |          .. code::\n",
      "     |  \n",
      "     |              <root>\n",
      "     |                   widerface\n",
      "     |                       wider_face_split ('wider_face_split.zip' if compressed)\n",
      "     |                       WIDER_train ('WIDER_train.zip' if compressed)\n",
      "     |                       WIDER_val ('WIDER_val.zip' if compressed)\n",
      "     |                       WIDER_test ('WIDER_test.zip' if compressed)\n",
      "     |      split (string): The dataset split to use. One of {``train``, ``val``, ``test``}.\n",
      "     |          Defaults to ``train``.\n",
      "     |      transform (callable, optional): A function/transform that  takes in a PIL image\n",
      "     |          and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "     |      target_transform (callable, optional): A function/transform that takes in the\n",
      "     |          target and transforms it.\n",
      "     |      download (bool, optional): If true, downloads the dataset from the internet and\n",
      "     |          puts it in root directory. If dataset is already downloaded, it is not\n",
      "     |          downloaded again.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WIDERFace\n",
      "     |      torchvision.datasets.vision.VisionDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "     |      Args:\n",
      "     |          index (int): Index\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple: (image, target) where target is a dict of annotations for all faces in the image.\n",
      "     |          target=None for the test split.\n",
      "     |  \n",
      "     |  __init__(self, root: str, split: str = 'train', transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self) -> int\n",
      "     |  \n",
      "     |  download(self) -> None\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |  \n",
      "     |  parse_test_annotations_file(self) -> None\n",
      "     |  \n",
      "     |  parse_train_val_annotations_file(self) -> None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ANNOTATIONS_FILE = ('http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/su...\n",
      "     |  \n",
      "     |  BASE_FOLDER = 'widerface'\n",
      "     |  \n",
      "     |  FILE_LIST = [('0B6eKvaijfFUDQUUwd21EckhUbWs', '3fedf70df600953d25982bc...\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torchvision.datasets.vision.VisionDataset:\n",
      "     |  \n",
      "     |  __repr__(self) -> str\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from builtins.type\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n",
      "DATA\n",
      "    __all__ = ('LSUN', 'LSUNClass', 'ImageFolder', 'DatasetFolder', 'FakeD...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.7/site-packages/torchvision/datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efbf12",
   "metadata": {},
   "source": [
    "#### Loading Fashion-MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6380b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1923ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"../data\",\n",
    "    train=True,\n",
    "    download=True, # if the data is saved in the root directory\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d3a5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = datasets.FashionMNIST(\n",
    "        root=\"../data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9abdd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module torchvision.datasets.mnist:\n",
      "\n",
      "__init__(self, root: str, train: bool = True, transform: Union[Callable, NoneType] = None, target_transform: Union[Callable, NoneType] = None, download: bool = False) -> None\n",
      "    Initialize self.  See help(type(self)) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.FashionMNIST.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e38ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __getitem__ in module torchvision.datasets.mnist:\n",
      "\n",
      "__getitem__(self, index: int) -> Tuple[Any, Any]\n",
      "    Args:\n",
      "        index (int): Index\n",
      "    \n",
      "    Returns:\n",
      "        tuple: (image, target) where target is index of the target class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.FashionMNIST.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36e540e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __len__ in module torchvision.datasets.mnist:\n",
      "\n",
      "__len__(self) -> int\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.FashionMNIST.__len__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40732c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t10k-images-idx3-ubyte\t   train-images-idx3-ubyte\r\n",
      "t10k-images-idx3-ubyte.gz  train-images-idx3-ubyte.gz\r\n",
      "t10k-labels-idx1-ubyte\t   train-labels-idx1-ubyte\r\n",
      "t10k-labels-idx1-ubyte.gz  train-labels-idx1-ubyte.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/FashionMNIST/raw/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d295912",
   "metadata": {},
   "source": [
    "### Iterating (using Dataloader) and Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e70bb7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a038190e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c7df6e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acaee330",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([19913])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(len(training_data), size=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a9b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, label = training_data[19913]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613fa1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc87bd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2946c4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\"\n",
    "}\n",
    "\n",
    "# matplotlib code to visualize one example for each\n",
    "figure = plt.figure(figsize=(8,8))\n",
    "cols,rows = 3,3\n",
    "for i in range(1,cols * rows + 1):\n",
    "    for img_index in range(len(training_data)):\n",
    "        if training_data[img_index][1] == i - 1:\n",
    "            img, label = training_data[img_index]\n",
    "            # indexing the subplot\n",
    "            figure.add_subplot(rows, cols, i)\n",
    "            plt.title(labels_map[label])\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(img.squeeze(),cmap=\"gray\")\n",
    "            break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d72ba577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# original lines of code\n",
    "# show random 9 images\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bef3c3",
   "metadata": {},
   "source": [
    "### Custom `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca43ef8",
   "metadata": {},
   "source": [
    "`torch.utils.data.Dataset` is inherited inside Custom Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cd2fa",
   "metadata": {},
   "source": [
    "`CustomDatasetClass` must contain  `__init__`, `__getitem__` and `__len__` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb5b44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torchvision.io as tvio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a15a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar to torchvision.datasets.FashionMNIST\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"FashionMNIST Image Dataset Class\"\"\"\n",
    "    def __init__(self, \n",
    "                 annotations_file,\n",
    "                 img_dir,\n",
    "                 transform=None,\n",
    "                 target_transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transform (Optional): dataset will take an optional argument transform \n",
    "                so that any required processing can be applied on the sample\n",
    "        \"\"\"\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # format of data\n",
    "        # tshirt1.jpg, 0\n",
    "        # tshirt2.jpg, 0\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx,0])\n",
    "        image = tvio.read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        sample = {\"image\": image, \"label\": label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59a027",
   "metadata": {},
   "source": [
    "### Training with DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35766c17",
   "metadata": {},
   "source": [
    "- `Dataset` retrieves features and labels one sample at a time\n",
    "What does `DataLoaders` do? <br>\n",
    "- For model training, <br>\n",
    "    - we need data fed in `minibatches` <br>\n",
    "    - reshuffle the data (to reduce model overfitting)\n",
    "    - `multiprocessing` to speed up data retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dcd204b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a832ef",
   "metadata": {},
   "source": [
    "How can we iterate through DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62804dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features.size()\n",
      "train_labels.size()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"train_features.size()\")\n",
    "print(f\"train_labels.size()\")\n",
    "\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[1]\n",
    "\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16eeba5",
   "metadata": {},
   "source": [
    "### Transforms in `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9b7d6",
   "metadata": {},
   "source": [
    "- Feature Transformation needed for `MNIST`: `Normalized Tensors` (using `ToTensor`)\n",
    "- Label Transformation: convert integer labels into one hot encoded tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7c2d5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"../data/\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(), # converts a Python Imaging Library (PIL) to float tensors within rag\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).\n",
    "                            scatter_(dim=0, # across rows\n",
    "                                     index=torch.tensor(y), \n",
    "                                    vaue=1) # scatter_ assigns a value=1 to the index mentioned in y\n",
    "                           )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15a16bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = Lambda(lambda y: torch.zeros(10, dtype=torch.float).\n",
    "                         scatter_(dim=0, index=torch.tensor(y), value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "102a61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [5, 6, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "92fffedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(10, dtype=torch.float).scatter_(dim=0, index=torch.tensor(labels), value=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
